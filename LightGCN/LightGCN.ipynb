{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import optim\nfrom scipy.sparse import csr_matrix\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.sparse as sp\nfrom time import time\nfrom torch import nn\nimport os\n#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:42:27.856787Z","start_time":"2022-01-27T02:42:25.901178Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:31.452118Z","iopub.execute_input":"2022-01-28T00:47:31.452521Z","iopub.status.idle":"2022-01-28T00:47:32.851995Z","shell.execute_reply.started":"2022-01-28T00:47:31.452482Z","shell.execute_reply":"2022-01-28T00:47:32.851259Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.copy('../input/lightgcn-test/LightGCN-PyTorch-master/data/gowalla/train.txt', './')\nshutil.copy('../input/lightgcn-test/LightGCN-PyTorch-master/data/gowalla/test.txt', './')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T00:47:32.853382Z","iopub.execute_input":"2022-01-28T00:47:32.853636Z","iopub.status.idle":"2022-01-28T00:47:33.144555Z","shell.execute_reply.started":"2022-01-28T00:47:32.853595Z","shell.execute_reply":"2022-01-28T00:47:33.143881Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:42:27.903148Z","start_time":"2022-01-27T02:42:27.858789Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:33.145834Z","iopub.execute_input":"2022-01-28T00:47:33.146236Z","iopub.status.idle":"2022-01-28T00:47:33.195785Z","shell.execute_reply.started":"2022-01-28T00:47:33.146193Z","shell.execute_reply":"2022-01-28T00:47:33.194922Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"'''\ndef set_seed(seed):\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.manual_seed(seed)\nset_seed(2020)\n'''","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:42:28.540882Z","start_time":"2022-01-27T02:42:28.512876Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataloader(Dataset):\n    def __init__(self, path='./'):\n        self.path = './'\n        trainUniqueUsers, trainItem, trainUser = [], [], []\n        testUniqueUsers, testItem, testUser = [], [], []\n        n_user, m_item = 0, 0\n        train_file = path + 'train.txt'\n        test_file = path + 'test.txt'\n        trainDataSize, testDataSize = 0, 0\n        with open(train_file) as f:\n            for l in f.readlines():\n                if len(l) > 0:\n                    l = l.strip('\\n').split(' ')\n                    items = [int(i) for i in l[1:]]\n                    uid = int(l[0])\n                    trainUniqueUsers.append([uid])\n                    trainUser.extend([uid] * len(items))\n                    trainItem.extend(items)\n                    m_item = max(m_item, max(items))\n                    n_user = max(n_user, uid)\n                    trainDataSize += len(items)\n\n        self.trainUniqueUsers = np.array(trainUniqueUsers)\n        self.trainUser = np.array(trainUser)\n        self.trainItem = np.array(trainItem)\n        self.trainDataSize = trainDataSize\n        \n        with open(test_file) as f:\n            for l in f.readlines():\n                if len(l) > 0:\n                    l = l.strip('\\n').split(' ')\n                    items = [int(i) for i in l[1:]]\n                    uid = int(l[0])\n                    testUser.extend([uid] * len(items))\n                    testItem.extend(items)\n                    m_item = max(m_item, max(items))\n                    n_user = max(n_user, uid)\n                    testDataSize += len(items)\n\n        self.testUniqueUsers = np.array(testUniqueUsers)\n        self.testUser = np.array(testUser)\n        self.testItem = np.array(testItem)\n        self.testDataSize = testDataSize\n        \n        self.n_user = n_user + 1\n        self.m_item = m_item + 1\n\n        print(f\"{self.trainDataSize} interactions for training\")\n        print(f\"{self.testDataSize} interactions for testing\")\n        print(f\"{self.n_user} users, {self.m_item} items\")\n        print(f\"Gowalla Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_user / self.m_item}\")\n        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n                                          shape=(self.n_user, self.m_item))\n        self.allPos = self.getAllPos(list(range(self.n_user)))\n        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n        self.users_D[self.users_D == 0.] = 1.\n        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n        self.items_D[self.items_D == 0.] = 1.\n        self.Graph = None\n        self.testDict = self.getTestDict()\n    \n\n    def getAllPos(self, users):\n        posItems = []\n        for user in users:\n            posItems.append(self.UserItemNet[user].nonzero()[1])\n        return posItems\n    \n    #采样\n    def uniformSample(self):\n        S = []\n        users = np.random.randint(0, self.n_user, self.trainDataSize)\n        for user in users:\n            posItemList = self.allPos[user]\n            if len(posItemList) == 0:\n                continue\n            posIndex = np.random.randint(0, len(posItemList))\n            posItem = posItemList[posIndex]\n            while True:\n                negItem = np.random.randint(0, self.m_item)\n                if negItem in posItemList:\n                    continue\n                else:\n                    break\n            S.append([user, posItem, negItem])\n        return np.array(S)\n    \n    \n    def _convert_sp_mat_to_sp_tensor(self, X):\n        coo = X.tocoo().astype(np.float32)\n        row = torch.Tensor(coo.row).long()\n        col = torch.Tensor(coo.col).long()\n        index = torch.stack([row, col])\n        data = torch.FloatTensor(coo.data)\n        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n    \n    def getSparseGraph(self):\n        if self.Graph is None:\n            try:\n                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n                print(\"successfully loaded adjacency matrix\")\n                norm_adj = pre_adj_mat\n            except :\n                print(\"generating adjacency matrix\")\n                s = time()\n                adj_mat = sp.dok_matrix((self.n_user + self.m_item, self.n_user + self.m_item), dtype=np.float32)\n                adj_mat = adj_mat.tolil()\n                R = self.UserItemNet.tolil()\n                adj_mat[:self.n_user, self.n_user:] = R\n                adj_mat[self.n_user:, :self.n_user] = R.T\n                adj_mat = adj_mat.todok()\n                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n                \n                rowsum = np.array(adj_mat.sum(axis=1))\n                d_inv = np.power(rowsum, -0.5).flatten()\n                d_inv[np.isinf(d_inv)] = 0.\n                d_mat = sp.diags(d_inv)\n                \n                norm_adj = d_mat.dot(adj_mat)\n                norm_adj = norm_adj.dot(d_mat)\n                norm_adj = norm_adj.tocsr()\n                end = time()\n                print(f\"costing {end-s}s, saved norm_mat...\")\n                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n            \n            self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n            self.Graph = self.Graph.coalesce().to(device)\n            print(\"don't split the matrix\")\n            \n        return self.Graph\n    \n    def getTestDict(self):\n        test_data = {}\n        for i, item in enumerate(self.testItem):\n            user = self.testUser[i]\n            if test_data.get(user):\n                test_data[user].append(item)\n            else:\n                test_data[user] = [item]\n        return test_data","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:03.689244Z","start_time":"2022-01-27T02:43:03.663244Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:33.199991Z","iopub.execute_input":"2022-01-28T00:47:33.200508Z","iopub.status.idle":"2022-01-28T00:47:33.232119Z","shell.execute_reply.started":"2022-01-28T00:47:33.200477Z","shell.execute_reply":"2022-01-28T00:47:33.231296Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class BPRLoss:\n    def __init__(self, model, lr=1e-3, weight_decay=1e-4):\n        self.model = model\n        self.weight_decay = weight_decay\n        self.lr = lr\n        self.opt = optim.Adam(model.parameters(), lr=self.lr)\n    \n    def stageOne(self,users, pos, neg):\n        loss, reg_loss = self.model.bpr_loss(users, pos, neg)\n        reg_loss = reg_loss*self.weight_decay\n        loss = loss + reg_loss\n\n        self.opt.zero_grad()\n        loss.backward()\n        self.opt.step()\n\n        return loss.cpu().item()","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:04.515065Z","start_time":"2022-01-27T02:43:04.497071Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:33.233709Z","iopub.execute_input":"2022-01-28T00:47:33.234316Z","iopub.status.idle":"2022-01-28T00:47:33.243603Z","shell.execute_reply.started":"2022-01-28T00:47:33.234277Z","shell.execute_reply":"2022-01-28T00:47:33.242928Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LightGCN(nn.Module):\n    def __init__(self, dataset, latent_dim=64, n_layers=3):\n        super(LightGCN, self).__init__()\n        self.dataset = dataset\n        \n        self.num_users = self.dataset.n_user\n        self.num_items = self.dataset.m_item\n        self.latent_dim = latent_dim\n        self.n_layers = n_layers\n        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n        nn.init.normal_(self.embedding_user.weight, std=0.1)\n        nn.init.normal_(self.embedding_item.weight, std=0.1)\n        \n        self.f = nn.Sigmoid()\n        self.Graph = self.dataset.getSparseGraph()\n        print(f\"lgn is already to go\")\n    \n    def computer(self):\n        users_emb = self.embedding_user.weight\n        items_emb = self.embedding_item.weight\n        g = self.Graph\n        all_emb = torch.cat([users_emb, items_emb])\n        embs = [all_emb]\n        \n        for layer in range(self.n_layers):\n            all_emb = torch.sparse.mm(g, all_emb)\n            embs.append(all_emb)\n            \n        embs = torch.stack(embs, dim=1)\n\n        light_out = torch.mean(embs, dim=1)\n\n        users, items = torch.split(light_out, [self.num_users, self.num_items])\n        return users, items\n\n    def getEmbedding(self, users, pos_items, neg_items):\n        all_users, all_items = self.computer()\n        users_emb = all_users[users]\n        pos_emb = all_items[pos_items]\n        neg_emb = all_items[neg_items]\n        users_emb_ego = self.embedding_user(users)\n        pos_emb_ego = self.embedding_item(pos_items)\n        neg_emb_ego = self.embedding_item(neg_items)\n        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n    \n    def bpr_loss(self, users, pos, neg):\n        (users_emb, pos_emb, neg_emb, \n        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n                         posEmb0.norm(2).pow(2)  +\n                         negEmb0.norm(2).pow(2))/float(len(users))\n        pos_scores = torch.mul(users_emb, pos_emb)\n        pos_scores = torch.sum(pos_scores, dim=1)\n        neg_scores = torch.mul(users_emb, neg_emb)\n        neg_scores = torch.sum(neg_scores, dim=1)\n        \n        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n        return loss, reg_loss\n    \n    def forward(self, users, items):\n        all_users, all_items = self.computer()\n        users_emb = all_users[users]\n        items_emb = all_items[items]\n        y_hat = torch.mul(users_emb, items_emb)\n        y_hat = y_hat.sum(dim=1)\n        return y_hat\n    \n    def getUserRating(self, users):\n        all_users, all_items = self.computer()\n        users_emb = all_users[users.long()]\n        items_emb = all_items\n        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n        return rating","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:04.764696Z","start_time":"2022-01-27T02:43:04.744692Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:33.245348Z","iopub.execute_input":"2022-01-28T00:47:33.245981Z","iopub.status.idle":"2022-01-28T00:47:33.265788Z","shell.execute_reply.started":"2022-01-28T00:47:33.245942Z","shell.execute_reply":"2022-01-28T00:47:33.264933Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_minibatch(batch_size, *tensors):\n    if len(tensors) == 1:\n        tensor = tensors[0]\n        for i in range(0, len(tensor), batch_size):\n            yield tensor[i: i+batch_size]\n    else:\n        for i in range(0, len(tensors[0]), batch_size):\n            yield tuple(x[i: i+batch_size] for x in tensors)\n    ","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:04.965318Z","start_time":"2022-01-27T02:43:04.96032Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:33.267552Z","iopub.execute_input":"2022-01-28T00:47:33.267729Z","iopub.status.idle":"2022-01-28T00:47:33.277506Z","shell.execute_reply.started":"2022-01-28T00:47:33.267705Z","shell.execute_reply":"2022-01-28T00:47:33.276770Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train(dataset, model, loss, epochs, batch_size=64):\n    \n    all_start = time()\n    \n    for epoch in range(epochs):\n        start = time()\n        model.train()\n        S = dataset.uniformSample()\n        users = torch.tensor(S[:, 0]).long().to(device)\n        posItems = torch.tensor(S[:, 1]).long().to(device)\n        negItems = torch.tensor(S[:, 2]).long().to(device)\n        \n        new_idx = np.random.permutation(len(users))\n        \n        users = users[new_idx]\n        posItems = posItems[new_idx]\n        negItems = negItems[new_idx]\n        \n        total_batch = len(users) // batch_size + 1\n        avg_loss = 0\n        \n        for (idx, (user, posItem, negItem)) in enumerate(get_minibatch(batch_size, users, posItems, negItems)):\n            avg_loss += loss.stageOne(user, posItem, negItem)\n        \n        avg_loss /= total_batch\n        end = time()\n        print(f'Epoch {epoch+1:^5d}/{epochs:^5d}: train loss {avg_loss:.5f}, cost {end-start:.1f}s')\n        if (epoch + 1) % 100 == 0:\n            torch.save(model.state_dict(), f'lgn-{epoch+1}-{avg_loss:.3f}.pkl')\n            K = 20\n            test_batch_size = 100\n            test(dataset, model, test_batch_size, K)\n            \n    print(f'All cost {time() - all_start: .1f}s')","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:05.294363Z","start_time":"2022-01-27T02:43:05.288288Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:33.280197Z","iopub.execute_input":"2022-01-28T00:47:33.280434Z","iopub.status.idle":"2022-01-28T00:47:33.291709Z","shell.execute_reply.started":"2022-01-28T00:47:33.280388Z","shell.execute_reply":"2022-01-28T00:47:33.290936Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset = Dataloader()","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:08.968541Z","start_time":"2022-01-27T02:43:05.728552Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:36.702865Z","iopub.execute_input":"2022-01-28T00:47:36.703348Z","iopub.status.idle":"2022-01-28T00:47:42.288412Z","shell.execute_reply.started":"2022-01-28T00:47:36.703310Z","shell.execute_reply":"2022-01-28T00:47:42.287653Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lgn_model = LightGCN(dataset).to(device)","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:09.223588Z","start_time":"2022-01-27T02:43:08.970529Z"},"execution":{"iopub.status.busy":"2022-01-28T00:47:42.290075Z","iopub.execute_input":"2022-01-28T00:47:42.290416Z","iopub.status.idle":"2022-01-28T00:49:51.129803Z","shell.execute_reply.started":"2022-01-28T00:47:42.290380Z","shell.execute_reply":"2022-01-28T00:49:51.128603Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"loss = BPRLoss(lgn_model)","metadata":{"ExecuteTime":{"end_time":"2022-01-27T02:43:09.238591Z","start_time":"2022-01-27T02:43:09.224589Z"},"execution":{"iopub.status.busy":"2022-01-28T00:49:51.131219Z","iopub.execute_input":"2022-01-28T00:49:51.131466Z","iopub.status.idle":"2022-01-28T00:49:51.135475Z","shell.execute_reply.started":"2022-01-28T00:49:51.131431Z","shell.execute_reply":"2022-01-28T00:49:51.134557Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def NDCG(test_data, r, K):\n    assert len(test_data) == len(r)\n    pred_data = r[:, :K]\n    test_matrix = np.zeros((len(pred_data), K))\n    for i, items in enumerate(test_data):\n        length = K if K <= len(items) else len(items)\n        test_matrix[i, :length] = 1\n    idcg = np.sum(test_matrix * 1.0 / np.log(np.arange(2, K+2)), axis=1)\n    dcg = np.sum(pred_data * 1.0 / np.log(np.arange(2, K+2)), axis=1)\n    idcg[idcg == 0.] = 1.\n    ndcg = dcg / idcg\n    ndcg[np.isnan(ndcg)] = 0.\n    return np.sum(ndcg)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T00:49:51.137658Z","iopub.execute_input":"2022-01-28T00:49:51.138227Z","iopub.status.idle":"2022-01-28T00:49:51.147213Z","shell.execute_reply.started":"2022-01-28T00:49:51.138187Z","shell.execute_reply":"2022-01-28T00:49:51.146510Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def test(dataset, model, batch_size, K):\n    testDict = dataset.testDict\n    model.eval()\n    precision, recall, ndcg = 0.0, 0.0, 0.0\n    user_cnt = 0\n    with torch.no_grad():\n        users = list(testDict.keys())\n        for batch_users in get_minibatch(batch_size, users):\n            user_cnt += len(batch_users)\n            allPos = dataset.getAllPos(batch_users)\n            groundTrue = [testDict[u] for u in batch_users]\n            \n            batch_users_gpu = torch.Tensor(batch_users).long()\n            batch_users_gpu = batch_users_gpu.to(device)\n            \n            rating = model.getUserRating(batch_users_gpu)\n            exclude_users = []\n            exclude_items = []\n            for i, items in enumerate(allPos):\n                exclude_users.extend(len(items) * [i])\n                exclude_items.extend(items)\n            \n            rating[exclude_users, exclude_items] = - (1<<10)\n            _, rating_K = torch.topk(rating, k=K)\n            rating_K = rating_K.cpu()\n\n            r, recall_n  = [], []\n            precision_n = K\n            for i, y in enumerate(groundTrue):\n                y_hat = rating_K[i]\n                pred = list(map(lambda x: x in y, y_hat))\n                pred = np.array(pred).astype('float')\n                r.append(pred)\n                recall_n.append(len(y))\n            r = np.array(r).astype('float')\n            ndcg += NDCG(groundTrue, r, K)\n            r = r[:, :K].sum(1)\n            recall_n = np.array(recall_n).astype('float')\n            precision += np.sum(r) / precision_n\n            recall += np.sum(r / recall_n)\n            \n            \n        precision /= user_cnt\n        recall /= user_cnt\n        ndcg /= user_cnt\n    print(f'Test precision: {precision:.4f}, recall: {recall:.4f}, ndcg: {ndcg:.4f}')\n            ","metadata":{"ExecuteTime":{"end_time":"2022-01-26T01:50:17.734327Z","start_time":"2022-01-26T01:50:17.716323Z"},"execution":{"iopub.status.busy":"2022-01-28T00:49:51.148669Z","iopub.execute_input":"2022-01-28T00:49:51.149267Z","iopub.status.idle":"2022-01-28T00:49:51.162992Z","shell.execute_reply.started":"2022-01-28T00:49:51.149229Z","shell.execute_reply":"2022-01-28T00:49:51.162007Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#batch_size = 2048\nbatch_size = 2048\nepochs = 1000\ntrain(dataset, lgn_model, loss, epochs, batch_size)","metadata":{"ExecuteTime":{"start_time":"2022-01-27T02:43:11.111Z"},"execution":{"iopub.status.busy":"2022-01-28T00:49:51.164632Z","iopub.execute_input":"2022-01-28T00:49:51.164994Z","iopub.status.idle":"2022-01-28T09:13:52.413715Z","shell.execute_reply.started":"2022-01-28T00:49:51.164897Z","shell.execute_reply":"2022-01-28T09:13:52.412870Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{"ExecuteTime":{"end_time":"2022-01-26T01:58:33.468431Z","start_time":"2022-01-26T01:58:10.118594Z"},"execution":{"iopub.status.busy":"2022-01-27T09:25:12.961488Z","iopub.execute_input":"2022-01-27T09:25:12.961903Z","iopub.status.idle":"2022-01-27T09:25:13.746278Z","shell.execute_reply.started":"2022-01-27T09:25:12.9618Z","shell.execute_reply":"2022-01-27T09:25:13.745274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}